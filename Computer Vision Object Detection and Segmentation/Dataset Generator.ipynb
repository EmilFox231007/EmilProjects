{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - making a dataset generator and patching the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "from patchify import patchify\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before patching the images, I cropped them using the task 2 code I made and had that as a function I could apply to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_petri_dish(image):\n",
    "\n",
    "    if len(image.shape) > 2:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "    # apply Gaussian Blur to smooth the image\n",
    "    blurred = cv2.GaussianBlur(gray_image, (9, 9), 0)\n",
    "    # apply binary threshold\n",
    "    _, thresh = cv2.threshold(blurred, 10, 255, cv2.THRESH_BINARY) \n",
    "    \n",
    "    # finding contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # filtering out very large contours which might be the image borders\n",
    "    contours = [cnt for cnt in contours if cv2.contourArea(cnt) < image.shape[0] * image.shape[1] * 0.95]\n",
    "\n",
    "    # findng the largest contour which will be the petri dish\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    \n",
    "    # modify the bounding rectangle to crop more tightly, the margin can be changed\n",
    "    margin = 90\n",
    "    x += margin\n",
    "    y += margin\n",
    "    w -= 2 * margin\n",
    "    h -= 2 * margin\n",
    "    \n",
    "    # ensuring the modified coordinates are within image bounds\n",
    "    x = max(x, 0)\n",
    "    y = max(y, 0)\n",
    "    w = min(w, image.shape[1] - x)\n",
    "    h = min(h, image.shape[0] - y)\n",
    "\n",
    "    # cropping the image based on calculated coordinates\n",
    "    cropped_image = image[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_image, (x, y, w, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then I made another function that would apply the same crooping coordinates used for the images but for the masks. Since the cropping function wouldn't work on the masks are there are not contours to find for cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_crop(image, bbox):\n",
    "    x, y, w, h = bbox\n",
    "    return image[y:y+h, x:x+w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then I padded the image using the code from self study notebook 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padder(image, patch_size):\n",
    "    \"\"\"\n",
    "    Adds padding to an image to make its dimensions divisible by a specified patch size.\n",
    "\n",
    "    This function calculates the amount of padding needed for both the height and width of an image so that its dimensions become divisible by the given patch size. The padding is applied evenly to both sides of each dimension (top and bottom for height, left and right for width). If the padding amount is odd, one extra pixel is added to the bottom or right side. The padding color is set to black (0, 0, 0).\n",
    "\n",
    "    Parameters:\n",
    "    - image (numpy.ndarray): The input image as a NumPy array. Expected shape is (height, width, channels).\n",
    "    - patch_size (int): The patch size to which the image dimensions should be divisible. It's applied to both height and width.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The padded image as a NumPy array with the same number of channels as the input. Its dimensions are adjusted to be divisible by the specified patch size.\n",
    "\n",
    "    Example:\n",
    "    - padded_image = padder(cv2.imread('example.jpg'), 128)\n",
    "\n",
    "    \"\"\"\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]\n",
    "    height_padding = ((h // patch_size) + 1) * patch_size - h\n",
    "    width_padding = ((w // patch_size) + 1) * patch_size - w\n",
    "\n",
    "    top_padding = int(height_padding/2)\n",
    "    bottom_padding = height_padding - top_padding\n",
    "\n",
    "    left_padding = int(width_padding/2)\n",
    "    right_padding = width_padding - left_padding\n",
    "\n",
    "    image = cv2.copyMakeBorder(image, top_padding, bottom_padding, left_padding, right_padding, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then comes the actual dataset generator, it takes the two directories, those being the original images and masks from the Y23 data set. For the masks it collects file paths that match the specified pattern (ex. \"*_root__mask\").  \n",
    "\n",
    "#### Then for each file path it will do the following: load each image and mask, it will resize the images and masks based on the set scaling factor which is 1.0. The apply the crop funciton to crop the images and mask. It will apply the padding function to each mask and image. Then it will use the patchify feature to patch the images and root masks with the set patch size, which at 128. Then it will extract the base name from each image and save them to a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patches_directly(dataset_type, patch_size, scaling_factor, output_dir, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Processes images and root masks, splits them into patches, and saves directly to disk without storing in memory.\n",
    "\n",
    "    Args:\n",
    "    - dataset_type (str): Type of dataset to process ('train', 'val').\n",
    "    - patch_size (int): Size of the patches to extract.\n",
    "    - scaling_factor (float): Scaling factor for resizing images and masks.\n",
    "    - output_dir (str): Directory to save the processed patches.\n",
    "    - train_ratio (float): Ratio of patches to allocate for training (default: 0.8).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    image_dir = r\"C:\\Users\\emilp\\Documents\\Buas_Y2\\Block_B\\data_sets\\Y-Combined_dataset\\total images\"\n",
    "    mask_dir = r\"C:\\Users\\emilp\\Documents\\Buas_Y2\\Block_B\\data_sets\\Y-Combined_dataset\\total masks\"\n",
    "\n",
    "    # Collect and sort paths for alignment\n",
    "    image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.png\")))\n",
    "    mask_root_paths = sorted(glob.glob(os.path.join(mask_dir, \"*.tif\")))\n",
    "\n",
    "    # Make directories\n",
    "    train_images_dir = os.path.join(output_dir, \"train_images\", \"images\")\n",
    "    train_masks_dir = os.path.join(output_dir, \"train_masks\", \"masks\")\n",
    "    val_images_dir = os.path.join(output_dir, \"val_images\", \"images\")\n",
    "    val_masks_dir = os.path.join(output_dir, \"val_masks\", \"masks\")\n",
    "\n",
    "    os.makedirs(train_images_dir, exist_ok=True)\n",
    "    os.makedirs(train_masks_dir, exist_ok=True)\n",
    "    os.makedirs(val_images_dir, exist_ok=True)\n",
    "    os.makedirs(val_masks_dir, exist_ok=True)\n",
    "\n",
    "    for img_path, root_path in zip(image_paths, mask_root_paths):\n",
    "        # Load and process images and root masks\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        root_mask = cv2.imread(root_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Resize\n",
    "        image = cv2.resize(image, (0, 0), fx=scaling_factor, fy=scaling_factor)\n",
    "        root_mask = cv2.resize(root_mask, (0, 0), fx=scaling_factor, fy=scaling_factor)\n",
    "\n",
    "        image, bbox = crop_petri_dish(image)\n",
    "        root_mask = apply_crop(root_mask, bbox)\n",
    "\n",
    "        image = padder(image, patch_size)\n",
    "        root_mask = padder(root_mask, patch_size)\n",
    "\n",
    "        # Patchify\n",
    "        image_patches = patchify(image, (patch_size, patch_size), step=patch_size).reshape(-1, patch_size, patch_size)\n",
    "        root_patches = patchify(root_mask, (patch_size, patch_size), step=patch_size).reshape(-1, patch_size, patch_size)\n",
    "\n",
    "        # Split into train and validation\n",
    "        indices = np.arange(len(image_patches))\n",
    "        train_indices, val_indices = train_test_split(indices, train_size=train_ratio, random_state=42)\n",
    "\n",
    "        for split, indices, img_dir, mask_dir in [\n",
    "            (\"train\", train_indices, train_images_dir, train_masks_dir),\n",
    "            (\"val\", val_indices, val_images_dir, val_masks_dir),\n",
    "        ]:\n",
    "            for idx in indices:\n",
    "                base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "                img_patch_path = os.path.join(img_dir, f\"{base_name}_patch_{idx}.png\")\n",
    "                mask_patch_path = os.path.join(mask_dir, f\"{base_name}_patch_{idx}.png\")\n",
    "\n",
    "                    # Write both image and mask patch tx`o disk\n",
    "                cv2.imwrite(img_patch_path, image_patches[idx].astype(np.uint8))\n",
    "                cv2.imwrite(mask_patch_path, root_patches[idx].astype(np.uint8) * 255)  # Enhance visibility for masks\n",
    "\n",
    "# Set parameters\n",
    "output_directory = r\"C:\\Users\\emilp\\Documents\\Buas_Y2\\Block_B\\data_sets\\Y-Combined_dataset\\placeholder_256\"\n",
    "dataset_type = \"train\"\n",
    "patch_size = 256\n",
    "scaling_factor = 1.0\n",
    "\n",
    "# Save patches directly\n",
    "save_patches_directly(dataset_type, patch_size, scaling_factor, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After which the list of original image names that was saved is used for the created patches. Every image generates around 500 patches, each patch having the same name as the original image with a number indicator at the end. It is also split into training and validation sets with a ratio of 80/20. And then saved into the requested folder structure. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block_d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
